Best practices to optimize Snowflake storage cost | by Samartha Chandrashekar | Snowflake | Oct, 2023 | MediumBest practices to optimize Snowflake storage costSamartha Chandrashekar·FollowPublished inSnowflake·6 min read·Oct 10--2ListenShareSnowflake is constantly improving the efficiency of storage mechanisms used behind the scenes.This post details the components of Snowflake storage costs and then covers best practices to help customers optimize storage spend on Snowflake over and above the ongoing behind-the-scenes improvements to storage efficiency delivered by Snowflake.Components of Snowflake storage costsFiles that are prepared for bulk data loading/unloading. These can be stored in compressed or uncompressed formats.Database tables include historical data for Time Travel (which enables accessing data that has been changed or deleted within a predefined period for restoring accidentally or intentionally deleted objects, backing up from points in the past, and analyzing past usage).Fail-safe for database tables (which provides a 7-day period after the Time Travel retention period during which historical data may be recovered).Clones of database tables that reference data deleted in the table that owns the clones. Snowflake minimizes the storage of historical data by maintaining only the information needed to restore updated/deleted table rows; full copies of tables are only maintained when tables are dropped or truncated.Usage is measured daily, and the monthly charge is calculated based on the average usage across the number of calendar days in the month. The monthly storage costs are determined by a flat rate per terabyte. The specific amount charged depends on the type of account and region. The fees for Time Travel and Fail-safe are calculated for each 24-hour period starting from the time the data was modified.The size displayed for a table represents the number of bytes that will be scanned if the entire table is queried; however, this number may be different from the number of physical bytes for the table, specifically for cloned tables and tables with deleted data. Additionally, data deleted from a table is not included in the displayed table size; however, the data is retained in Snowflake until both the Time Travel retention period (default is 1 day) and the Fail-safe period (7 days) for the data have passed. During these two periods, the displayed table size is smaller than the actual physical bytes stored for the table.Snowflake optimizes storage for customers with compression, which can help reduce storage volume by up to 7x. Snowflake charges customers based on compressed storage volume.Best practices to optimize storage costsOur best practices to optimize storage volume and thus costs include:1/ Explore and understand storage costsUsers with the ACCOUNTADMIN role can view the amount of data stored either via the UI (Storage filter under the Usage Type drop-down) or execute queries against the following views ACCOUNT_USAGE and/or ORGANIZATION_USAGE:DATABASE_STORAGE_USAGE_HISTORY which shows the average daily storage in bytes for each database in the account/organization.LISTING_AUTO_FULFILLMENT_ DATABASE_STORAGE_DAILY which shows data storage in bytes for databases fulfilled to other regions by cross-cloud auto-fulfillment.STORAGE_DAILY_HISTORY which shows the average daily storage for storage in bytes. It combines database storage (DATABASE_STORAGE_USAGE_HISTORY) and stage storage (STAGE_STORAGE_USAGE_HISTORY).STAGE_STORAGE_USAGE_HISTORY which shows the average daily storage usage, in bytes, for all the Snowflake stages including named internal stages and default staging areas.TABLE_STORAGE_METRICS which contains details on storage in bytes for tables, including storage that is no longer active but continues to incur costs (e.g., deleted tables with the Time Travel retention period).USAGE_IN_CURRENCY_DAILY which contains details on the daily average storage in bytes along with the cost of that usage in the organization’s currency.The USAGE_IN_CURRENCY_DAILY view shows cost in currency instead of size, by converting the storage size into cost in currency by using the daily price of a terabytes.2/ Use temporary and transient table types to control storage cost implications of Time Travel and Fail-safeStorage is calculated and charged for data regardless of whether it is in Active, Time Travel, or Fail-safe states. Additionally, updated or deleted data protected by these features continue to incur storage costs until the data leaves the Fail-safe state.To help manage the storage costs implications of Time Travel and Fail-safe capabilities, Snowflake provides ‘temporary’ and ‘transient’ table types in addition to the default table type of ‘permanent’.Transient tables exist until explicitly dropped, after which the historical data beyond the Time Travel retention period cannot be recovered. Temporary tables only exist for the lifetime of their associated session, after which data is purged and unrecoverable. Transient and temporary tables don’t come with a Fail-safe period and can have a Time Travel retention period of either 0 or 1 day — which means they can, at most, incur one day’s worth of storage cost.Additionally, it is noteworthy that TIME_TRAVEL_BYTES and FAILSAFE_BYTES incur charges when data is loaded using INSERT, COPY, or SNOWPIPE. This is because the defragmentation of micro-partition deletes small micro-partitions and creates a new one with the same data. The deleted micro-partitions contribute to TIME_TRAVEL_BYTES and FAILSAFE_BYTES.3/ Use appropriate data protection strategies for high-churn dimension tables and low-churn fact tablesFact tables typically have large sizes and low churn (with inserts of new data or deletions of older data). This makes Time Travel and Fail-safe protection at a relatively low cost. As a result, for long-lived tables, such as fact tables or reference data, we recommend using the ‘permanent’ table type to ensure full protection by Fail-safe.Dimension tables are usually high churn with lots of row updates and deletions which cause underlying micro-partitions to undergo life-cycle transitions. This has the potential to cause the storage associated with Time Travel and Fail-safe to be much larger than the active table storage.High-churn dimension tables can be identified if the ratio of FAILSAFE_BYTES to ACTIVE_BYTES in the TABLE_STORAGE_METRICS view is large. As a result, there is a need to consider the cost and benefits of Time Travel and Fail-safe for such dimension tables. In many cases, it can be a better strategy to use the ‘transient’ type with zero Time Travel retention and periodically copy contents into a permanent table for a full backup; old backups can be deleted when a new one is created.Similarly, for short-lived tables (e.g., ETL work tables that need to stay for less than a day), we recommend using ‘transient’ or ‘temporary’ table types to minimize Fail-safe costs.4/ Use zero-copy cloning for backupsSnowflake’s zero-copy cloning provides a convenient way to “snapshot” any table, schema, or database and create a derived copy that initially shares the underlying storage. This helps with instant backups that do not incur additional costs.Each clone has its own separate life cycle and changes can be made to the original object or clone independently. There are no limits on the number of clones that can be created, which can result in an n-level hierarchy of cloned objects with shared and independent storage.A cloned table does not use additional storage until rows are added to or modified/deleted from the table. The displayed table size may be larger than the actual physical bytes stored for the table; as a result, the cloned table contributes less to the overall storage than the size indicates.5/ Periodically delete older data based on age thresholdsBased on the needs of your use case, deleting unneeded data can help control storage costs.You can run delete commands to remove rows in tables data older than age thresholds that meet the needs of your use case. This can be automated by using Snowflake tasks or by scripting custom lifecycle policies on matching rows on permanent tables, internal stages (directory tables), managed iceberg tables, hybrid tables, etc. These custom scripted lifecycle policies or Snowflake tasks can be run at a cadence that makes sense for your use case.Based on the needs for user role based segmentation of privileges for your use case, it may be necessary to allow granular permissions for these policies; for example, only administrators can define storage lifecycle policies to delete data while table owners can apply these policies on their tables, monitor policy executions, and track policy execution history.6/ Consider using Apache Iceberg based on your use caseSnowflake supports the Apache Iceberg file format that eliminates the need to move or copy tables between different systems, which often translates to lower storage (and compute) costs for your overall data stack.Iceberg tables managed by Snowflake offer similar performance to ingested Snowflake-format tables. Furthermore, you can seamlessly plug Snowflake into other Iceberg catalogs tracking table metadata. If your use case can tolerate the small performance difference across Iceberg tables and Snowflake-format tables, consider using Iceberg to save storage costs.Conclusion and recapTo recap, our best practices for optimizing Snowflake storage costs are: 1/ Explore and understand storage costs, 2/ Use temporary and transient table types to control storage costs, 3/ Use appropriate data protection strategy for high-churn dimension tables and low-churn fact tables, 4/ Use zero-copy cloning for backups, 5/ Periodically delete older data based on age thresholds, 6/ Consider using Apache Iceberg based on the use case.Cost OptimizationSnowflakeStorageCloud ComputingPerformance----2FollowWritten by Samartha Chandrashekar113 Followers·Writer for SnowflakeProduct Managment at SnowflakeFollowMore from Samartha Chandrashekar and SnowflakeSamartha ChandrashekarinSnowflakeDeep dive into the internals of Snowflake Virtual WarehousesSnowflake’s Data Cloud provided as Software-as-a-Service (SaaS), enables data storage, processing, analytic solutions, Machine Learning…27 min read·Sep 21--Vino Duraisamy | Views my own.inSnowflakeStep by step roadmap to becoming a Snowflake Data Engineer in 2023Hint — SQL & Python is all you need. Yeah, no kidding!!4 min read·Aug 15--6Cristian ScutaruinSnowflakeStreamlit ERD Viewer in SnowflakeA while ago I wrote about an Entity-Relationship Diagram (ERD) generator for Snowflake, implemented as a separate proof-of-concept Python…5 min read·Sep 22--2Samartha ChandrashekarinSnowflakeBest practices to optimize Snowflake spendSnowflake is constantly improving the value of a Snowflake credit via ongoing transparent platform improvements. Customers are…9 min read·Aug 10--See all from Samartha ChandrashekarSee all from SnowflakeRecommended from MediumMilan MosnyinInfostrux Engineering BlogWhy Snowflake External Network Access Changes EverythingLearn about the impact that this seemingly simple feature can have4 min read·Oct 2--2Somen SwaininSnowflake3 brand new ways to derive query insights in SnowflakeAssessing the workload within Snowflake is a very important especially considering the fact that the warehouses used for running any query…4 min read·Oct 14--ListsA Guide to Choosing, Planning, and Achieving Personal Goals13 stories·560 savesStaff Picks484 stories·376 savesNatural Language Processing743 stories·332 savesMiguel Bayan SoaresinMinderaDatabricks VS SnowflakeWith so many technologies surfacing all the time in the data landscape, it’s becoming very hard to understand where we should invest our…3 min read·Oct 10--DataGeeks❄️We reduced our Snowflake cost by 50%❄️Snowflake is a robust and scalable cloud data warehousing platform that allows businesses to store, process, and analyze large volumes of…·8 min read·Oct 3--2Data Engineering SimplifiedSnowflake Schema Detection Feature for JSON FilesData loading processes are crucial for timely insights and decision-making. Snowflake, has add its support for JSON using “Infer Schema”…5 min read·Aug 16--Caleb H.Snowflake Dashboard: Credit Usage By UserA simple overview attributing warehouse credit consumption to individual user credit usage in an informal calculation10 min read·Oct 13--See more recommendationsHelpStatusAboutCareersBlogPrivacyTermsText to speechTeams
































